{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cbb06a3-62be-4bd4-ae18-b7e0696e54a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium beautifulsoup4 webdriver_manager transformers torch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66fc3b63-bf44-466c-b619-7d2fea7f9d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "def setup_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--mute-audio\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "# Initialize driver\n",
    "driver = setup_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d35ef63-d250-48da-9049-6fd636b04cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Load the YouTube video URL\n",
    "url = \"https://youtu.be/WKijw2dpFDA?si=EQteR6n1XfoCIYuQ\"  # Replace VIDEO_ID with the actual video ID\n",
    "driver.get(url)\n",
    "time.sleep(5)  # Wait for the page to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7676ebfd-363f-4616-95fe-b6ca6918d9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: HOLY SH*T! India Drops BOMBSHELL On Justin Trudeau\n"
     ]
    }
   ],
   "source": [
    "def scrape_title(driver):\n",
    "    try:\n",
    "        # Locate the title using the provided structure\n",
    "        title_element = driver.find_element(By.CSS_SELECTOR, '#title yt-formatted-string')\n",
    "        title = title_element.text\n",
    "        return title\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return None\n",
    "\n",
    "# Test the modified scrape_title function\n",
    "title = scrape_title(driver)\n",
    "print(\"Title:\", title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "411d61de-9d92-45f8-98b1-0a3d92a2d2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Views: 178601\n",
      "Upload Date: 2 Nov 2024\n",
      "Tags: ['#pierrepoilievre', '#justintrudeau', '#canada']\n"
     ]
    }
   ],
   "source": [
    "import re  # Ensure re is imported\n",
    "\n",
    "def scrape_views_date_tags(driver):\n",
    "    try:\n",
    "        # Try to expand the description box, if the button is available\n",
    "        try:\n",
    "            show_more = driver.find_element(By.CSS_SELECTOR, 'tp-yt-paper-button#expand')\n",
    "            driver.execute_script(\"arguments[0].click();\", show_more)\n",
    "            time.sleep(2)  # Wait for content to load\n",
    "        except Exception as e:\n",
    "            print(\"No 'Show more' button found or already expanded.\")\n",
    "\n",
    "        # Locate the info section containing views, upload date, and tags\n",
    "        info_element = driver.find_element(By.CSS_SELECTOR, 'yt-formatted-string#info')\n",
    "        \n",
    "        # Extract views count using regex\n",
    "        views_match = re.search(r'(\\d+[.,]?\\d*[K|M|B]?) views', info_element.text)\n",
    "        if views_match:\n",
    "            views_str = views_match.group(1).replace(',', '').replace('K', '000').replace('M', '000000').replace('B', '000000000')\n",
    "            views_count = int(float(views_str))\n",
    "        else:\n",
    "            views_count = None\n",
    "\n",
    "        # Extract upload date using adjusted regex to match formats like \"2 Nov 2024\" or \"Nov 3, 2024\"\n",
    "        date_match = re.search(r'(\\d{1,2}\\s\\w+\\s\\d{4}|\\w+\\s\\d{1,2},\\s\\d{4})', info_element.text)\n",
    "        upload_date = date_match.group(1) if date_match else None\n",
    "\n",
    "        # Extract tags (hashtags) within the info section\n",
    "        tag_elements = info_element.find_elements(By.CSS_SELECTOR, 'a.yt-simple-endpoint')\n",
    "        tags = [tag.text for tag in tag_elements]\n",
    "\n",
    "        return views_count, upload_date, tags\n",
    "    except Exception as e:\n",
    "        print(\"Error scraping views, upload date, or tags:\", e)\n",
    "        return None, None, []\n",
    "\n",
    "# Test scrape_views_date_tags function\n",
    "views_count, upload_date, tags = scrape_views_date_tags(driver)\n",
    "print(\"Views:\", views_count)\n",
    "print(\"Upload Date:\", upload_date)\n",
    "print(\"Tags:\", tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29a11ebc-9056-4ea0-bc25-170d36ddb45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likes: 5016\n"
     ]
    }
   ],
   "source": [
    "def scrape_likes(driver):\n",
    "    try:\n",
    "        # Locate the like button using its class name and get the aria-label attribute for the like count\n",
    "        like_button = driver.find_element(By.CSS_SELECTOR, 'button[aria-label*=\"like this video\"]')\n",
    "        \n",
    "        # Extract the like count from aria-label (e.g., \"like this video along with 4,995 other people\")\n",
    "        likes_text = like_button.get_attribute(\"aria-label\")\n",
    "        likes_match = re.search(r'(\\d+[.,]?\\d*[K|M|B]?)', likes_text)\n",
    "        \n",
    "        if likes_match:\n",
    "            # Convert shorthand notations (e.g., \"K\", \"M\") to full numbers\n",
    "            likes_str = likes_match.group(1).replace(',', '').replace('K', '000').replace('M', '000000').replace('B', '000000000')\n",
    "            likes_count = int(float(likes_str))\n",
    "        else:\n",
    "            likes_count = None\n",
    "\n",
    "        # Dislikes are not publicly available, so set it to None\n",
    "        dislikes_count = None\n",
    "        \n",
    "        return likes_count, dislikes_count\n",
    "    except Exception as e:\n",
    "        print(\"Error scraping likes or dislikes:\", e)\n",
    "        return None, None\n",
    "\n",
    "# Test scrape_likes function\n",
    "likes_count, dislikes_count = scrape_likes(driver)\n",
    "print(\"Likes:\", likes_count)\n",
    "#print(\"Dislikes:\", dislikes_count)  # Will be None since dislikes are hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d897cf58-9ba5-41d1-9079-4c58abdab5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel Name: mistersunshinebaby\n",
      "Subscriber Count: 421000\n"
     ]
    }
   ],
   "source": [
    "import re  # Ensure re is imported for regex operations\n",
    "\n",
    "def scrape_channel_info(driver):\n",
    "    try:\n",
    "        # Locate the channel name\n",
    "        channel_name_element = driver.find_element(By.CSS_SELECTOR, '#channel-name #text')\n",
    "        channel_name = channel_name_element.text.strip()\n",
    "\n",
    "        # Locate the subscriber count and process it\n",
    "        subscriber_count_element = driver.find_element(By.CSS_SELECTOR, '#owner-sub-count')\n",
    "        subscriber_count_text = subscriber_count_element.text.strip().replace(\"subscribers\", \"\").strip()  # Remove \"subscribers\" word\n",
    "\n",
    "        # Convert shorthand notation to integer\n",
    "        if 'K' in subscriber_count_text:\n",
    "            subscriber_count = int(float(subscriber_count_text.replace('K', '')) * 1000)\n",
    "        elif 'M' in subscriber_count_text:\n",
    "            subscriber_count = int(float(subscriber_count_text.replace('M', '')) * 1000000)\n",
    "        else:\n",
    "            subscriber_count = int(subscriber_count_text.replace(',', ''))  # Direct conversion if no shorthand\n",
    "\n",
    "        return channel_name, subscriber_count\n",
    "    except Exception as e:\n",
    "        print(\"Error scraping channel name or subscriber count:\", e)\n",
    "        return None, None\n",
    "\n",
    "# Test scrape_channel_info function\n",
    "channel_name, subscriber_count = scrape_channel_info(driver)\n",
    "print(\"Channel Name:\", channel_name)\n",
    "print(\"Subscriber Count:\", subscriber_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09020c6f-b3b7-458e-b96c-3e5d08ba9bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript saved to transcript.json\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Set up Chrome options and service\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "service = Service(ChromeDriverManager().install())\n",
    "\n",
    "# Set up WebDriver\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def scrape_transcript(driver):\n",
    "    try:\n",
    "        # Expand the description if necessary\n",
    "        try:\n",
    "            more_button = driver.find_element(By.XPATH, '//*[@id=\"expand\"]')\n",
    "            driver.execute_script(\"arguments[0].click();\", more_button)\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(\"No 'More' button found or already expanded.\")\n",
    "\n",
    "        # Click the transcript button\n",
    "        transcript_button = driver.find_element(By.XPATH, '//*[@id=\"primary-button\"]/ytd-button-renderer/yt-button-shape/button')\n",
    "        transcript_button.click()\n",
    "        time.sleep(2)  # Wait for the transcript to load\n",
    "\n",
    "        # Initialize an empty string to store the full transcript\n",
    "        transcript = \"\"\n",
    "\n",
    "        # Loop through all transcript segments\n",
    "        index = 1\n",
    "        while True:\n",
    "            try:\n",
    "                # Locate each transcript segment by constructing the XPath dynamically\n",
    "                parent_xpath = f'//*[@id=\"segments-container\"]/ytd-transcript-segment-renderer[{index}]'\n",
    "                child_xpath = f'{parent_xpath}/div/yt-formatted-string'\n",
    "                \n",
    "                # Extract the text of each segment\n",
    "                text_element = driver.find_element(By.XPATH, child_xpath)\n",
    "                transcript += text_element.text + \"\\n\"\n",
    "                \n",
    "                # Move to the next segment\n",
    "                index += 1\n",
    "            except Exception as e:\n",
    "                # Break the loop when no more segments are found\n",
    "                break\n",
    "\n",
    "        return transcript\n",
    "    except Exception as e:\n",
    "        print(\"Error scraping transcript:\", e)\n",
    "        return None\n",
    "\n",
    "# Usage example\n",
    "video_url = 'https://www.youtube.com/watch?v=WKijw2dpFDA'  # Example YouTube video URL\n",
    "driver.get(video_url)\n",
    "time.sleep(5)  # Wait for the page to load fully\n",
    "\n",
    "# Call the function to scrape the transcript\n",
    "transcript_text = scrape_transcript(driver)\n",
    "\n",
    "# Save the transcript to a JSON file\n",
    "if transcript_text:\n",
    "    transcript_data = {\"transcript\": transcript_text}\n",
    "    with open(\"transcript.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(transcript_data, json_file, ensure_ascii=False, indent=4)\n",
    "    print(\"Transcript saved to transcript.json\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aacd121-db43-46d0-9b8e-6aa00c6dd27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Comments: 1269\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re\n",
    "\n",
    "def scrape_comment_count(driver):\n",
    "    try:\n",
    "        # Scroll to load the comments section\n",
    "        driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "        time.sleep(2)  # Give some time for the comments section to load\n",
    "\n",
    "        # Wait until the comment count element is present\n",
    "        comment_count_element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//yt-formatted-string[@class=\"count-text style-scope ytd-comments-header-renderer\"]'))\n",
    "        )\n",
    "\n",
    "        # Extract the number of comments\n",
    "        comment_count_text = comment_count_element.text\n",
    "        comment_count = int(re.sub('[^0-9]', '', comment_count_text))  # Remove non-numeric characters and convert to integer\n",
    "\n",
    "        return comment_count\n",
    "    except Exception as e:\n",
    "        print(\"Error scraping comment count:\", e)\n",
    "        return None\n",
    "\n",
    "# Test scrape_comment_count function\n",
    "comment_count = scrape_comment_count(driver)\n",
    "print(\"Number of Comments:\", comment_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bd83d49-116b-4e24-9ef9-fb6ed9f967ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments section did not load within the wait time.\n"
     ]
    },
    {
     "ename": "MaxRetryError",
     "evalue": "HTTPConnectionPool(host='localhost', port=54908): Max retries exceeded with url: /session/f63e251cb4b233788c285feb2698c7cd/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x125535010>: Failed to establish a new connection: [Errno 61] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:196\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[1;32m    197\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[1;32m    199\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[1;32m    200\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[1;32m    201\u001b[0m     )\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    790\u001b[0m     conn,\n\u001b[1;32m    791\u001b[0m     method,\n\u001b[1;32m    792\u001b[0m     url,\n\u001b[1;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    802\u001b[0m )\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:495\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 495\u001b[0m     conn\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    496\u001b[0m         method,\n\u001b[1;32m    497\u001b[0m         url,\n\u001b[1;32m    498\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    499\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    500\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    501\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    502\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    503\u001b[0m         enforce_content_length\u001b[38;5;241m=\u001b[39menforce_content_length,\n\u001b[1;32m    504\u001b[0m     )\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:398\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[0;32m--> 398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders()\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:1091\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1091\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1094\u001b[0m \n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:1035\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m-> 1035\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:236\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:211\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    213\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Audit hooks are only available in Python 3.8+\u001b[39;00m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x125535010>: Failed to establish a new connection: [Errno 61] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Scroll once to load initial batch of comments\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m scroll_once(driver)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Function to scrape comments, likes, and reply counts\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_comments\u001b[39m(driver, max_comments\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n",
      "Cell \u001b[0;32mIn[27], line 38\u001b[0m, in \u001b[0;36mscroll_once\u001b[0;34m(driver)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscroll_once\u001b[39m(driver):\n\u001b[0;32m---> 38\u001b[0m     driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow.scrollTo(0, document.documentElement.scrollHeight);\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:414\u001b[0m, in \u001b[0;36mWebDriver.execute_script\u001b[0;34m(self, script, *args)\u001b[0m\n\u001b[1;32m    411\u001b[0m converted_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(args)\n\u001b[1;32m    412\u001b[0m command \u001b[38;5;241m=\u001b[39m Command\u001b[38;5;241m.\u001b[39mW3C_EXECUTE_SCRIPT\n\u001b[0;32m--> 414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(command, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscript\u001b[39m\u001b[38;5;124m\"\u001b[39m: script, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: converted_args})[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:352\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m    350\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[0;32m--> 352\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py:306\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    304\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[1;32m    305\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(command_info[\u001b[38;5;241m0\u001b[39m], url, body\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py:326\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    323\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 326\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m    327\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/_request_methods.py:144\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[1;32m    137\u001b[0m         method,\n\u001b[1;32m    138\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[1;32m    142\u001b[0m     )\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[1;32m    145\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[1;32m    146\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/_request_methods.py:279\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    275\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, content_type)\n\u001b[1;32m    277\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/poolmanager.py:443\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, u\u001b[38;5;241m.\u001b[39mrequest_uri, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    445\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:873\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    872\u001b[0m     )\n\u001b[0;32m--> 873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    874\u001b[0m         method,\n\u001b[1;32m    875\u001b[0m         url,\n\u001b[1;32m    876\u001b[0m         body,\n\u001b[1;32m    877\u001b[0m         headers,\n\u001b[1;32m    878\u001b[0m         retries,\n\u001b[1;32m    879\u001b[0m         redirect,\n\u001b[1;32m    880\u001b[0m         assert_same_host,\n\u001b[1;32m    881\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    882\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[1;32m    883\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[1;32m    884\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    885\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[1;32m    886\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    887\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    889\u001b[0m     )\n\u001b[1;32m    891\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[1;32m    892\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:873\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    872\u001b[0m     )\n\u001b[0;32m--> 873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    874\u001b[0m         method,\n\u001b[1;32m    875\u001b[0m         url,\n\u001b[1;32m    876\u001b[0m         body,\n\u001b[1;32m    877\u001b[0m         headers,\n\u001b[1;32m    878\u001b[0m         retries,\n\u001b[1;32m    879\u001b[0m         redirect,\n\u001b[1;32m    880\u001b[0m         assert_same_host,\n\u001b[1;32m    881\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    882\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[1;32m    883\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[1;32m    884\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    885\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[1;32m    886\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    887\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    889\u001b[0m     )\n\u001b[1;32m    891\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[1;32m    892\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:873\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    872\u001b[0m     )\n\u001b[0;32m--> 873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    874\u001b[0m         method,\n\u001b[1;32m    875\u001b[0m         url,\n\u001b[1;32m    876\u001b[0m         body,\n\u001b[1;32m    877\u001b[0m         headers,\n\u001b[1;32m    878\u001b[0m         retries,\n\u001b[1;32m    879\u001b[0m         redirect,\n\u001b[1;32m    880\u001b[0m         assert_same_host,\n\u001b[1;32m    881\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    882\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[1;32m    883\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[1;32m    884\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    885\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[1;32m    886\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    887\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    889\u001b[0m     )\n\u001b[1;32m    891\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[1;32m    892\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_e, (\u001b[38;5;167;01mOSError\u001b[39;00m, HTTPException)):\n\u001b[1;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[1;32m    844\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    845\u001b[0m )\n\u001b[1;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n\u001b[1;32m    848\u001b[0m \u001b[38;5;66;03m# Keep track of the error for the retry warning.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[1;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_retry\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=54908): Max retries exceeded with url: /session/f63e251cb4b233788c285feb2698c7cd/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x125535010>: Failed to establish a new connection: [Errno 61] Connection refused'))"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Setup Selenium options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Open YouTube video page\n",
    "video_url = 'https://www.youtube.com/watch?v=WKijw2dpFDA'  # Replace with your YouTube video URL\n",
    "driver.get(video_url)\n",
    "\n",
    "# Initial scroll to trigger comment loading\n",
    "driver.execute_script(\"window.scrollTo(0, 800);\")\n",
    "time.sleep(3)\n",
    "\n",
    "# Extended Wait for comments section\n",
    "try:\n",
    "    WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"ytd-comment-thread-renderer\"))\n",
    "    )\n",
    "except:\n",
    "    print(\"Comments section did not load within the wait time.\")\n",
    "    driver.quit()\n",
    "\n",
    "# Scroll function with limited single scroll attempt\n",
    "def scroll_once(driver):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    time.sleep(3)\n",
    "\n",
    "# Scroll once to load initial batch of comments\n",
    "scroll_once(driver)\n",
    "\n",
    "# Function to scrape comments, likes, and reply counts\n",
    "def scrape_comments(driver, max_comments=3):\n",
    "    comments_data = []\n",
    "    comments_elements = driver.find_elements(By.CSS_SELECTOR, \"ytd-comment-thread-renderer\")\n",
    "\n",
    "    for comment_element in comments_elements[:max_comments]:\n",
    "        try:\n",
    "            # Extract comment text\n",
    "            comment_text = comment_element.find_element(By.CSS_SELECTOR, \"#content-text\").text\n",
    "            \n",
    "            # Extract like count for the comment\n",
    "            try:\n",
    "                likes = comment_element.find_element(By.CSS_SELECTOR, \"#vote-count-middle\").text\n",
    "                likes = int(likes.replace('K', '000').replace('M', '000000'))\n",
    "            except:\n",
    "                likes = 0\n",
    "\n",
    "            # Extract reply count if available\n",
    "            try:\n",
    "                reply_button = comment_element.find_element(By.CSS_SELECTOR, \"#more-replies\")\n",
    "                replies_text = reply_button.get_attribute(\"aria-label\")\n",
    "                replies_count = int(replies_text.split()[0].replace('K', '000').replace('M', '000000'))\n",
    "            except:\n",
    "                replies_count = 0\n",
    "\n",
    "            # Append data to list\n",
    "            comments_data.append({\n",
    "                \"comment_text\": comment_text,\n",
    "                \"likes\": likes,\n",
    "                \"replies\": replies_count\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error scraping comment:\", e)\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# Scrape the comments\n",
    "comments = scrape_comments(driver, max_comments=3)\n",
    "\n",
    "# Save data to JSON file\n",
    "with open(\"youtube_comments.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(comments, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Output confirmation\n",
    "print(\"Scraping complete. Data saved to youtube_comments.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f7b1931-f926-4f1e-970e-7a39cd5f0bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install youtube-comment-downloader --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9797faed-de15-4415-b977-8ee7c056bb77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\u200b\\u200b\\u200b\\u200bGo to https://piavpn.com/Sunshine to get 83% off Private Internet Access with 4 months free! ðŸš€',\n",
       " \"How much is Jagmeet tied to this? He can't set foot in India, or he'll be arrested. He and trudeau are propping each other up for a reason.\",\n",
       " 'I am so sorry India. We the people are not behind  our current government. We have asked Trudeau repeatedly to leave. Soon, we all hope.',\n",
       " 'The Butcher of Canada AKA Justin Trudeau is a global embarrassment',\n",
       " \"As A Indian Living in India let me tell you few facts..\\n1. The Sikhs in Canada the Khalistani movement is to separate a State of India Which is Punjab and Make it a Different country \\n2.The Khalistani people were behind the A Flight Bomb blast in 1980s\\n3.The Sikhs living in India are peaceful and proud to be Indians\\n4.The Indian Government since decades have sent various Notices that these are designated Terrorist living in Canada to send them to India but Trudeau Goverment never cooperated with India\\n5.Majority of Khalistani in Canada have Different Gangs and involved in various Drugs and Gang Wars Criminal Activities \\n6. As A Indian The Canadian Citizen I know are unaware of the actual matter so no need to Apologise \\nPeace'\",\n",
       " 'Canadians are with you India!  We want Trudeau GONE.',\n",
       " 'Jag  keeps saying that a Canadian was killed by the Indian Gov. What was this Canadian doing medaling in another countries affairs?',\n",
       " 'Honestly I understand why India is only bashing liedeau so is the entire world',\n",
       " 'Trudeau makes me SICK',\n",
       " 'India should file a criminal case against Trudeau and then seek his extradition to India to face the charges in an Indian court. That will put Trudeau in his right place.',\n",
       " 'At least India gives canadians the real news',\n",
       " 'In canada leaking china information is criminal but leaking India information is strategic plan ðŸ˜‚ðŸ˜‚ðŸ˜‚',\n",
       " \"South african here ðŸ‡¿ðŸ‡¦. Listening to the fact that school children are being taught about sexuality and some have undergone gender change surgery is absolutely disgusting ðŸ¤¢ and disturbing to me. If that were to happen here in RSA, the teacher and or doctor will be lynched in public. No mentally sane nation would even let something like this become a mainstream talking point of politics. \\n\\nAbout the India issue, Nijjar was already wanted by interpol themselves with 2 red notices out for him also khalistanis are largely responsible for the rise of drug and gang violence in Canada. India warned the Canadian government about this 2 years ago yet the Trudeau government did nothing. The Khalistanis have been confirmed by both the Brazilian and Indian governments to have an involvement in the South American drug cartels. \\n\\nSo hypothetically if India did kill Nijjar, shouldn't Canadians be thankful to India as they've done more to combat crime in Canada than the actual Canadian government?.\\n\\nAs far as I see it, Trudeau is picking a fight with India to garner khalistanis vote bank and to distract from other problems like high house prices, food inflation and rising crime and violence in Canada also the accusations of China interfering in Canadian elections is yet to be resolved.\",\n",
       " 'Hey Freeland, can we cancel Trudeau? He is more expensive than Disney Plus!',\n",
       " 'Trudeau= Canadian Pappu ðŸƒ ðŸ¤¡',\n",
       " \"Why can't kids just be kids? I don't understand why sexuality and gender labels are something our children and youth struggle with today. This wasn't an issue when I was a kid. I feel sorry for our kids today.\",\n",
       " 'Why is it ONLY adults with NO children, trying to tell us how to raise yours.',\n",
       " 'Love our amazing Premier Danielle Smith.â¤ï¸',\n",
       " \"Wow Not only do Canadians can't stand him, JT is burning bridges everywhere goes.\",\n",
       " 'Iâ€™m a former liberal voter but what a total disaster JT has been. Weâ€™re sorry India. Please put JT in his place. \\nSuspended democracy, deemed unethical, total embarrassment for Canada to say the least.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from youtube_comment_downloader import YoutubeCommentDownloader, SORT_BY_POPULAR\n",
    "import traceback\n",
    "import csv\n",
    "import logging\n",
    "\n",
    "class ScraperComments:\n",
    "    def __init__(self, url, sort_by=SORT_BY_POPULAR, comment_limit=10):\n",
    "        # Ensure the full URL is passed directly without modification\n",
    "        self.url = url\n",
    "        self.sort_by = sort_by\n",
    "        self.comment_limit = comment_limit\n",
    "        self.comments = []\n",
    "\n",
    "    def get_comments(self):\n",
    "        \"\"\"Scrapes comments from a YouTube video URL.\"\"\"\n",
    "        try:\n",
    "            downloader = YoutubeCommentDownloader()\n",
    "            comments = downloader.get_comments_from_url(self.url, sort_by=self.sort_by)\n",
    "            comment_data = []\n",
    "            for comment in islice(comments, self.comment_limit):\n",
    "                comment_data.append(comment['text'])\n",
    "            logging.info(\"Successfully downloaded comments\")\n",
    "            return comment_data\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error in downloading comments\", exc_info=True)\n",
    "            return None\n",
    "\n",
    "    def save_to_csv(self, comments, file_name='comments.csv'):\n",
    "        \"\"\"Saves scraped comments to a CSV file.\"\"\"\n",
    "        if not comments:\n",
    "            logging.warning(\"No comments to save.\")\n",
    "            return\n",
    "        try:\n",
    "            with open(file_name, 'w', newline='', encoding='utf-8') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(['Comment'])\n",
    "                for comment in comments:\n",
    "                    writer.writerow([comment])\n",
    "            logging.info(f\"Comments saved to {file_name}\")\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error in saving comments to CSV\", exc_info=True)\n",
    "\n",
    "    def scrape_and_save_comments(self):\n",
    "        \"\"\"Main function to scrape and save comments.\"\"\"\n",
    "        comments = self.get_comments()\n",
    "        self.save_to_csv(comments)\n",
    "        return comments\n",
    "# Usage Example:\n",
    "scraper = ScraperComments(\"https://www.youtube.com/watch?v=WKijw2dpFDA\", comment_limit=20)\n",
    "scraper.scrape_and_save_comments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8868ca89-db0d-407b-8474-3ca464f82e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved at comments.json\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from youtube_comment_downloader import YoutubeCommentDownloader, SORT_BY_POPULAR\n",
    "import logging\n",
    "import json\n",
    "\n",
    "class ScraperComments:\n",
    "    def __init__(self, url, sort_by=SORT_BY_POPULAR, comment_limit=10):\n",
    "        # Ensure the full URL is passed directly without modification\n",
    "        self.url = url\n",
    "        self.sort_by = sort_by\n",
    "        self.comment_limit = comment_limit\n",
    "        self.comments = []\n",
    "\n",
    "    def get_comments(self):\n",
    "        \"\"\"Scrapes comments from a YouTube video URL.\"\"\"\n",
    "        try:\n",
    "            downloader = YoutubeCommentDownloader()\n",
    "            comments = downloader.get_comments_from_url(self.url, sort_by=self.sort_by)\n",
    "            comment_data = []\n",
    "            for comment in islice(comments, self.comment_limit):\n",
    "                comment_data.append(comment['text'])\n",
    "            logging.info(\"Successfully downloaded comments\")\n",
    "            return comment_data\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error in downloading comments\", exc_info=True)\n",
    "            return None\n",
    "\n",
    "    def save_to_json(self, comments, file_name='comments.json'):\n",
    "        \"\"\"Saves scraped comments to a JSON file.\"\"\"\n",
    "        if not comments:\n",
    "            logging.warning(\"No comments to save.\")\n",
    "            return\n",
    "        try:\n",
    "            with open(file_name, 'w', encoding='utf-8') as file:\n",
    "                json.dump(comments, file, ensure_ascii=False, indent=4)\n",
    "            logging.info(f\"Comments saved to {file_name}\")\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error in saving comments to JSON\", exc_info=True)\n",
    "\n",
    "    def scrape_and_save_comments(self):\n",
    "        \"\"\"Main function to scrape and save comments.\"\"\"\n",
    "        comments = self.get_comments()\n",
    "        self.save_to_json(comments)  # Save comments to JSON\n",
    "        #return comments\n",
    "\n",
    "# Usage Example:\n",
    "scraper = ScraperComments(\"https://www.youtube.com/watch?v=WKijw2dpFDA\", comment_limit=21)\n",
    "scraper.scrape_and_save_comments()\n",
    "print(\"Successfully saved at comments.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cfe5c0-c3bf-4a27-a657-a829596d1f49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
