{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cda440a-67f6-4c20-8197-d9aba0dfe52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Using cached selenium-4.25.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Using cached trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (2024.6.2)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (1.8.0)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Using cached selenium-4.25.0-py3-none-any.whl (9.7 MB)\n",
      "Downloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: h11, attrs, wsproto, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "Successfully installed attrs-24.2.0 h11-0.14.0 outcome-1.3.0.post0 selenium-4.25.0 trio-0.27.0 trio-websocket-0.11.1 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b505be3c-be3b-44ef-8eb6-346d58faf48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:99: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:99: SyntaxWarning: invalid escape sequence '\\['\n",
      "/var/folders/qz/3374k0z123qd44rjjjhkmxnr0000gn/T/ipykernel_11296/2294538391.py:99: SyntaxWarning: invalid escape sequence '\\['\n",
      "  keywords_match = re.search('\"keywords\":\\[(.*?)\\]', page_source)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter YouTube video URL:  https://youtu.be/9D0bGia4QrI?si=TEXzWYIN3YpZqzJy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: \n",
      "\n",
      "description: None\n",
      "\n",
      "views: 758,263 views\n",
      "\n",
      "date_published: None\n",
      "\n",
      "likes: Like this comment along with 384 other people\n",
      "\n",
      "channel_name: Sherlock Holmes Stories Magpie Audio\n",
      "\n",
      "subscriber_count: 150K subscribers\n",
      "\n",
      "duration: 50:42\n",
      "\n",
      "tags: ['scandal in bohemia', 'Scandal', 'Bohemia', 'Adventures of Sherlock Holmes', 'Sherlock Holmes', 'Holmes', 'Homes', 'Watson', 'detective', 'Greg Wagland', 'Wagland', 'Magpie Audio', 'unabridged']\n",
      "\n",
      "comments: 0 comments scraped\n",
      "\n",
      "comment_count: 273 Comments\n",
      "\n",
      "transcript: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "\n",
    "def scrape_youtube_video(url):\n",
    "    # Set up Selenium WebDriver options\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Run headless Chrome\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--window-size=1920x1080\")\n",
    "    options.add_argument(\"--mute-audio\")\n",
    "    \n",
    "    # Initialize the WebDriver (make sure to specify the path to chromedriver if necessary)\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Open the YouTube video URL\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "    # Scroll to load dynamic content\n",
    "    driver.execute_script(\"window.scrollTo(0, 600);\")  # Scroll to load description and likes\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Parse the page source with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    video_data = {}\n",
    "\n",
    "    # Scrape the title\n",
    "    try:\n",
    "        title = soup.find('h1', {'class': 'title'}).text.strip()\n",
    "    except:\n",
    "        title = None\n",
    "    video_data['title'] = title\n",
    "\n",
    "    # Scrape the description\n",
    "    try:\n",
    "        # Expand the description if necessary\n",
    "        show_more = driver.find_element(By.XPATH, \"//tp-yt-paper-button[@id='expand']\")\n",
    "        driver.execute_script(\"arguments[0].click();\", show_more)\n",
    "        time.sleep(2)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        description = soup.find('yt-formatted-string', {'class': 'content', 'slot': 'content'}).text.strip()\n",
    "    except:\n",
    "        description = None\n",
    "    video_data['description'] = description\n",
    "\n",
    "    # Scrape the number of views\n",
    "    try:\n",
    "        views = soup.find('span', class_='view-count').text.strip()\n",
    "    except:\n",
    "        views = None\n",
    "    video_data['views'] = views\n",
    "\n",
    "    # Scrape the published date\n",
    "    try:\n",
    "        date_published = soup.find('div', {'id': 'date'}).find('yt-formatted-string').text.strip()\n",
    "    except:\n",
    "        date_published = None\n",
    "    video_data['date_published'] = date_published\n",
    "\n",
    "    # Scrape the number of likes\n",
    "    try:\n",
    "        # Likes are dynamically loaded; need to scroll and wait\n",
    "        driver.execute_script(\"window.scrollTo(0, 800);\")\n",
    "        time.sleep(2)\n",
    "        like_button = driver.find_element(By.XPATH, \"//ytd-toggle-button-renderer[1]//a\")\n",
    "        likes = like_button.get_attribute('aria-label')\n",
    "    except:\n",
    "        likes = None\n",
    "    video_data['likes'] = likes\n",
    "\n",
    "    # Scrape the uploader information\n",
    "    try:\n",
    "        channel_name = soup.find('yt-formatted-string', {'class': 'ytd-channel-name'}).find('a').text.strip()\n",
    "    except:\n",
    "        channel_name = None\n",
    "    video_data['channel_name'] = channel_name\n",
    "\n",
    "    try:\n",
    "        subscriber_count = soup.find('yt-formatted-string', {'id': 'owner-sub-count'}).text.strip()\n",
    "    except:\n",
    "        subscriber_count = None\n",
    "    video_data['subscriber_count'] = subscriber_count\n",
    "\n",
    "    # Scrape the video duration\n",
    "    try:\n",
    "        duration = soup.find('span', {'class': 'ytp-time-duration'}).text.strip()\n",
    "    except:\n",
    "        duration = None\n",
    "    video_data['duration'] = duration\n",
    "\n",
    "    # Scrape tags (keywords) from page source\n",
    "    try:\n",
    "        page_source = driver.page_source\n",
    "        keywords_match = re.search('\"keywords\":\\[(.*?)\\]', page_source)\n",
    "        if keywords_match:\n",
    "            keywords = keywords_match.group(1).replace('\"', '').split(',')\n",
    "        else:\n",
    "            keywords = None\n",
    "    except:\n",
    "        keywords = None\n",
    "    video_data['tags'] = keywords\n",
    "\n",
    "    # Scrape comments\n",
    "    try:\n",
    "        # Scroll to the comments section\n",
    "        driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "        time.sleep(5)  # Wait for comments to load\n",
    "        # Scroll multiple times to load more comments\n",
    "        last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "            new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        comment_elems = soup.find_all('yt-formatted-string', {'id': 'content-text'})\n",
    "        comments = [comment.text.strip() for comment in comment_elems]\n",
    "    except:\n",
    "        comments = []\n",
    "    video_data['comments'] = comments\n",
    "\n",
    "    # Scrape the number of comments\n",
    "    try:\n",
    "        comment_count = soup.find('h2', {'id': 'count'}).find('yt-formatted-string').text.strip()\n",
    "    except:\n",
    "        comment_count = None\n",
    "    video_data['comment_count'] = comment_count\n",
    "\n",
    "    # Scrape the transcript\n",
    "    try:\n",
    "        # Click on the \"More actions\" button (the three dots)\n",
    "        more_actions = driver.find_element(By.XPATH, \"//button[@aria-label='More actions']\")\n",
    "        driver.execute_script(\"arguments[0].click();\", more_actions)\n",
    "        time.sleep(1)\n",
    "        # Click on \"Open transcript\"\n",
    "        open_transcript = driver.find_element(By.XPATH, \"//yt-formatted-string[text()='Show transcript']\")\n",
    "        driver.execute_script(\"arguments[0].click();\", open_transcript)\n",
    "        time.sleep(2)\n",
    "        # Extract the transcript\n",
    "        transcript_elements = driver.find_elements(By.XPATH, \"//ytd-transcript-renderer//div[@id='body']//div[@class='cue-group style-scope ytd-transcript-body-renderer']//div[@class='cue style-scope ytd-transcript-body-renderer']\")\n",
    "        transcript = ' '.join([elem.text.strip() for elem in transcript_elements])\n",
    "    except:\n",
    "        transcript = None\n",
    "    video_data['transcript'] = transcript\n",
    "\n",
    "    # Close the driver\n",
    "    driver.quit()\n",
    "\n",
    "    return video_data\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = input(\"Enter YouTube video URL: \")\n",
    "    data = scrape_youtube_video(url)\n",
    "    for key, value in data.items():\n",
    "        if key == 'comments':\n",
    "            print(f\"{key}: {len(value)} comments scraped\\n\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "968fe668-66c0-4966-b316-33fe1d01333f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:100: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:100: SyntaxWarning: invalid escape sequence '\\['\n",
      "/var/folders/qz/3374k0z123qd44rjjjhkmxnr0000gn/T/ipykernel_11296/801765162.py:100: SyntaxWarning: invalid escape sequence '\\['\n",
      "  keywords_match = re.search('\"keywords\":\\[(.*?)\\]', page_source)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter YouTube video URL:  https://youtu.be/9D0bGia4QrI?si=TEXzWYIN3YpZqzJy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: \n",
      "\n",
      "description: None\n",
      "\n",
      "views: 758,266 views\n",
      "\n",
      "date_published: None\n",
      "\n",
      "likes: Like this comment along with 384 other people\n",
      "\n",
      "channel_name: Sherlock Holmes Stories Magpie Audio\n",
      "\n",
      "subscriber_count: 150K subscribers\n",
      "\n",
      "duration: 50:42\n",
      "\n",
      "tags: ['scandal in bohemia', 'Scandal', 'Bohemia', 'Adventures of Sherlock Holmes', 'Sherlock Holmes', 'Holmes', 'Homes', 'Watson', 'detective', 'Greg Wagland', 'Wagland', 'Magpie Audio', 'unabridged']\n",
      "\n",
      "comments: 0 comments scraped\n",
      "\n",
      "comment_count: 273 Comments\n",
      "\n",
      "transcript: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "\n",
    "def scrape_youtube_video(url):\n",
    "    # Set up Chrome options and service\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Runs Chrome in headless mode\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "\n",
    "    # Set up WebDriver\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    # Open YouTube video\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for page to load\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Scroll to load dynamic content\n",
    "    driver.execute_script(\"window.scrollTo(0, 600);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Parse the page source with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    video_data = {}\n",
    "\n",
    "    # Scrape the title\n",
    "    try:\n",
    "        title = soup.find('h1', {'class': 'title'}).text.strip()\n",
    "    except:\n",
    "        title = None\n",
    "    video_data['title'] = title\n",
    "\n",
    "    # Scrape the description\n",
    "    try:\n",
    "        # Expand the description if necessary\n",
    "        more_button = driver.find_element(By.XPATH, '//*[@id=\"expand\"]')\n",
    "        driver.execute_script(\"arguments[0].click();\", more_button)\n",
    "        time.sleep(2)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        description = soup.find('yt-formatted-string', {'class': 'content', 'slot': 'content'}).text.strip()\n",
    "    except:\n",
    "        description = None\n",
    "    video_data['description'] = description\n",
    "\n",
    "    # Scrape the number of views\n",
    "    try:\n",
    "        views = soup.find('span', class_='view-count').text.strip()\n",
    "    except:\n",
    "        views = None\n",
    "    video_data['views'] = views\n",
    "\n",
    "    # Scrape the published date\n",
    "    try:\n",
    "        date_published = soup.find('div', {'id': 'date'}).find('yt-formatted-string').text.strip()\n",
    "    except:\n",
    "        date_published = None\n",
    "    video_data['date_published'] = date_published\n",
    "\n",
    "    # Scrape the number of likes\n",
    "    try:\n",
    "        like_button = driver.find_element(By.XPATH, \"//ytd-toggle-button-renderer[1]//a\")\n",
    "        likes = like_button.get_attribute('aria-label')\n",
    "    except:\n",
    "        likes = None\n",
    "    video_data['likes'] = likes\n",
    "\n",
    "    # Scrape the uploader information\n",
    "    try:\n",
    "        channel_name = soup.find('yt-formatted-string', {'class': 'ytd-channel-name'}).find('a').text.strip()\n",
    "    except:\n",
    "        channel_name = None\n",
    "    video_data['channel_name'] = channel_name\n",
    "\n",
    "    try:\n",
    "        subscriber_count = soup.find('yt-formatted-string', {'id': 'owner-sub-count'}).text.strip()\n",
    "    except:\n",
    "        subscriber_count = None\n",
    "    video_data['subscriber_count'] = subscriber_count\n",
    "\n",
    "    # Scrape the video duration\n",
    "    try:\n",
    "        duration = soup.find('span', {'class': 'ytp-time-duration'}).text.strip()\n",
    "    except:\n",
    "        duration = None\n",
    "    video_data['duration'] = duration\n",
    "\n",
    "    # Scrape tags (keywords) from page source\n",
    "    try:\n",
    "        page_source = driver.page_source\n",
    "        keywords_match = re.search('\"keywords\":\\[(.*?)\\]', page_source)\n",
    "        if keywords_match:\n",
    "            keywords = keywords_match.group(1).replace('\"', '').split(',')\n",
    "        else:\n",
    "            keywords = None\n",
    "    except:\n",
    "        keywords = None\n",
    "    video_data['tags'] = keywords\n",
    "\n",
    "    # Scrape comments\n",
    "    try:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "        time.sleep(5)  # Wait for comments to load\n",
    "        last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "            new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        comment_elems = soup.find_all('yt-formatted-string', {'id': 'content-text'})\n",
    "        comments = [comment.text.strip() for comment in comment_elems]\n",
    "    except:\n",
    "        comments = []\n",
    "    video_data['comments'] = comments\n",
    "\n",
    "    # Scrape the number of comments\n",
    "    try:\n",
    "        comment_count = soup.find('h2', {'id': 'count'}).find('yt-formatted-string').text.strip()\n",
    "    except:\n",
    "        comment_count = None\n",
    "    video_data['comment_count'] = comment_count\n",
    "\n",
    "    # Scrape the transcript\n",
    "    try:\n",
    "        # Open the transcript menu\n",
    "        more_actions = driver.find_element(By.XPATH, \"//button[@aria-label='More actions']\")\n",
    "        driver.execute_script(\"arguments[0].click();\", more_actions)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Click on \"Open transcript\"\n",
    "        open_transcript = driver.find_element(By.XPATH, \"//yt-formatted-string[text()='Show transcript']\")\n",
    "        driver.execute_script(\"arguments[0].click();\", open_transcript)\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Extract the transcript\n",
    "        transcript = \"\"\n",
    "        index = 1\n",
    "        while True:\n",
    "            try:\n",
    "                # Locate each transcript segment\n",
    "                parent_xpath = f'//*[@id=\"segments-container\"]/ytd-transcript-segment-renderer[{index}]'\n",
    "                child_xpath = f'{parent_xpath}/div/yt-formatted-string'\n",
    "                text_element = driver.find_element(By.XPATH, child_xpath)\n",
    "                transcript += text_element.text + \"\\n\"\n",
    "                index += 1\n",
    "            except:\n",
    "                break\n",
    "    except:\n",
    "        transcript = None\n",
    "    video_data['transcript'] = transcript\n",
    "\n",
    "    # Close the driver\n",
    "    driver.quit()\n",
    "\n",
    "    return video_data\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = input(\"Enter YouTube video URL: \")\n",
    "    data = scrape_youtube_video(url)\n",
    "    for key, value in data.items():\n",
    "        if key == 'comments':\n",
    "            print(f\"{key}: {len(value)} comments scraped\\n\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5be1092-522a-4037-8b4f-240cc59ad8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter YouTube video URL:  https://youtu.be/9D0bGia4QrI?si=TEXzWYIN3YpZqzJy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: 1 A Scandal in Bohemia from The Adventures of Sherlock Holmes  (1892) Audiobook\n",
      "\n",
      "description: None\n",
      "\n",
      "views: 758,269 views\n",
      "\n",
      "date_published: 2017-12-16T14:30:01-08:00\n",
      "\n",
      "likes: None\n",
      "\n",
      "channel_name: Sherlock Holmes Stories Magpie Audio\n",
      "\n",
      "subscriber_count: 150K subscribers\n",
      "\n",
      "duration: 50:43\n",
      "\n",
      "tags: ['scandal in bohemia', 'Scandal', 'Bohemia', 'Adventures of Sherlock Holmes', 'Sherlock Holmes', 'Holmes', 'Homes', 'Watson', 'detective', 'Greg Wagland', 'Wagland', 'Magpie Audio', 'unabridged']\n",
      "\n",
      "comments: 0 comments scraped\n",
      "\n",
      "comment_count: None\n",
      "\n",
      "transcript: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "\n",
    "def scrape_youtube_video(url):\n",
    "    # Set up Chrome options and service\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Runs Chrome in headless mode\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--window-size=1920x1080\")\n",
    "    chrome_options.add_argument(\"--mute-audio\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "\n",
    "    # Set up WebDriver\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    # Open YouTube video\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for page to load\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Scroll to load dynamic content\n",
    "    driver.execute_script(\"window.scrollTo(0, 600);\")  # Scroll to load description and likes\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Parse the page source with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    video_data = {}\n",
    "\n",
    "    # Scrape the title\n",
    "    try:\n",
    "        # Try to get title from meta tag\n",
    "        title = soup.find('meta', property='og:title')['content']\n",
    "    except:\n",
    "        try:\n",
    "            # Fallback to h1\n",
    "            title = soup.find('h1').text.strip()\n",
    "        except:\n",
    "            title = None\n",
    "    video_data['title'] = title\n",
    "\n",
    "    # Scrape the description\n",
    "    try:\n",
    "        # Expand the description if necessary\n",
    "        more_button = driver.find_element(By.XPATH, \"//tp-yt-paper-button[@id='expand']\")\n",
    "        driver.execute_script(\"arguments[0].click();\", more_button)\n",
    "        time.sleep(2)\n",
    "        # Update soup after clicking\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        description = soup.find('yt-formatted-string', {'class': 'content', 'slot': 'content'}).text.strip()\n",
    "    except:\n",
    "        description = None\n",
    "    video_data['description'] = description\n",
    "\n",
    "    # Scrape the number of views\n",
    "    try:\n",
    "        # Get views from meta tag\n",
    "        views = soup.find('meta', itemprop='interactionCount')['content']\n",
    "        # Format views with commas\n",
    "        views = f\"{int(views):,} views\"\n",
    "    except:\n",
    "        try:\n",
    "            # Fallback to span\n",
    "            views = soup.find('span', class_='view-count').text.strip()\n",
    "        except:\n",
    "            views = None\n",
    "    video_data['views'] = views\n",
    "\n",
    "    # Scrape the published date\n",
    "    try:\n",
    "        date_published = soup.find('meta', itemprop='datePublished')['content']\n",
    "    except:\n",
    "        try:\n",
    "            date_published = soup.find('div', {'id': 'date'}).find('yt-formatted-string').text.strip()\n",
    "        except:\n",
    "            date_published = None\n",
    "    video_data['date_published'] = date_published\n",
    "\n",
    "    # Scrape the number of likes\n",
    "    try:\n",
    "        # Like button's aria-label contains likes\n",
    "        like_button = driver.find_element(By.XPATH, \"//ytd-toggle-button-renderer[1]//yt-formatted-string\")\n",
    "        likes = like_button.get_attribute('aria-label')\n",
    "        # Extract number from string\n",
    "        likes_number = re.search(r'([\\d,]+)', likes)\n",
    "        if likes_number:\n",
    "            likes = likes_number.group(1)\n",
    "    except:\n",
    "        likes = None\n",
    "    video_data['likes'] = likes\n",
    "\n",
    "    # Scrape the uploader information\n",
    "    try:\n",
    "        channel_name = soup.find('yt-formatted-string', {'class': 'ytd-channel-name'}).find('a').text.strip()\n",
    "    except:\n",
    "        channel_name = None\n",
    "    video_data['channel_name'] = channel_name\n",
    "\n",
    "    try:\n",
    "        subscriber_count = soup.find('yt-formatted-string', {'id': 'owner-sub-count'}).text.strip()\n",
    "    except:\n",
    "        subscriber_count = None\n",
    "    video_data['subscriber_count'] = subscriber_count\n",
    "\n",
    "    # Scrape the video duration\n",
    "    try:\n",
    "        duration = soup.find('meta', itemprop='duration')['content']\n",
    "        # Convert duration from ISO 8601 to HH:MM:SS\n",
    "        match = re.match(r'PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?', duration)\n",
    "        hours = int(match.group(1)) if match.group(1) else 0\n",
    "        minutes = int(match.group(2)) if match.group(2) else 0\n",
    "        seconds = int(match.group(3)) if match.group(3) else 0\n",
    "        duration = f\"{hours}:{minutes:02}:{seconds:02}\" if hours > 0 else f\"{minutes}:{seconds:02}\"\n",
    "    except:\n",
    "        duration = None\n",
    "    video_data['duration'] = duration\n",
    "\n",
    "    # Scrape tags (keywords) from page source\n",
    "    try:\n",
    "        page_source = driver.page_source\n",
    "        # Correct regex with escaped brackets\n",
    "        keywords_match = re.search(r'\"keywords\":\\[(.*?)\\]', page_source)\n",
    "        if keywords_match:\n",
    "            keywords = keywords_match.group(1).replace('\"', '').split(',')\n",
    "            keywords = [tag.strip() for tag in keywords]\n",
    "        else:\n",
    "            keywords = None\n",
    "    except:\n",
    "        keywords = None\n",
    "    video_data['tags'] = keywords\n",
    "\n",
    "    # Scrape comments\n",
    "    try:\n",
    "        # Scroll to the comments section\n",
    "        driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "        time.sleep(5)  # Wait for comments to load\n",
    "\n",
    "        # Scroll multiple times to load more comments\n",
    "        last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "        scroll_attempt = 0\n",
    "        while scroll_attempt < 5:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "            new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "            scroll_attempt += 1\n",
    "\n",
    "        # Update soup after scrolling\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        comment_elems = soup.find_all('yt-formatted-string', {'id': 'content-text'})\n",
    "        comments = [comment.text.strip() for comment in comment_elems]\n",
    "    except:\n",
    "        comments = []\n",
    "    video_data['comments'] = comments\n",
    "\n",
    "    # Scrape the number of comments\n",
    "    try:\n",
    "        # Get comment count from meta tag\n",
    "        comment_count = soup.find('yt-formatted-string', {'id': 'count'}).text.strip()\n",
    "    except:\n",
    "        comment_count = None\n",
    "    video_data['comment_count'] = comment_count\n",
    "\n",
    "    # Scrape the transcript using the user's working code\n",
    "    try:\n",
    "        # Click on the \"More actions\" button (the three dots)\n",
    "        more_actions = driver.find_element(By.XPATH, \"//button[@aria-label='More actions']\")\n",
    "        driver.execute_script(\"arguments[0].click();\", more_actions)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Click on \"Show transcript\"\n",
    "        open_transcript = driver.find_element(By.XPATH, \"//yt-formatted-string[text()='Show transcript']\")\n",
    "        driver.execute_script(\"arguments[0].click();\", open_transcript)\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Initialize an empty string to store the full transcript\n",
    "        transcript = \"\"\n",
    "\n",
    "        # Start from the first parent container and loop through all transcript segments\n",
    "        index = 1\n",
    "        while True:\n",
    "            try:\n",
    "                # Format the parent and child XPath using the index\n",
    "                parent_xpath = f'//*[@id=\"segments-container\"]/ytd-transcript-segment-renderer[{index}]'\n",
    "                child_xpath = f'{parent_xpath}/div/yt-formatted-string'\n",
    "\n",
    "                # Locate the child element that contains the transcript text\n",
    "                text_element = driver.find_element(By.XPATH, child_xpath)\n",
    "                transcript += text_element.text + \"\\n\"\n",
    "\n",
    "                # Move to the next parent container\n",
    "                index += 1\n",
    "            except:\n",
    "                # Break the loop if no more segments are found\n",
    "                break\n",
    "    except:\n",
    "        transcript = None\n",
    "    video_data['transcript'] = transcript\n",
    "\n",
    "    # Close the driver\n",
    "    driver.quit()\n",
    "\n",
    "    return video_data\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = input(\"Enter YouTube video URL: \")\n",
    "    data = scrape_youtube_video(url)\n",
    "    for key, value in data.items():\n",
    "        if key == 'comments':\n",
    "            print(f\"{key}: {len(value)} comments scraped\\n\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1738e09-df35-460a-9022-40ba234cc8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
